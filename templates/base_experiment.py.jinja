"""
Auto-generated experiment script.
Experiment: {{ experiment_name }}
Generated at: {{ timestamp }}
"""

import json
import sys
import warnings
from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    mean_squared_error,
    mean_absolute_error,
    r2_score,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    log_loss,
)

warnings.filterwarnings('ignore')

# Configuration
DATA_PATH = "{{ data_path }}"
TARGET_COLUMN = "{{ target_column }}"
TASK_TYPE = "{{ task_type }}"
RANDOM_STATE = 42
TEST_SIZE = 0.2

{% block imports %}
{% endblock %}

def load_data():
    """Load and prepare the dataset."""
    df = pd.read_csv(DATA_PATH)
    X = df.drop(columns=[TARGET_COLUMN])
    y = df[TARGET_COLUMN]
    return X, y


def create_preprocessor(X):
    """Create preprocessing pipeline."""
    # Identify column types
    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

    # Numeric preprocessing
    {% if preprocessing.missing_values == 'drop' %}
    numeric_imputer = 'passthrough'  # Will drop rows with missing values later
    {% elif preprocessing.missing_values == 'mean' %}
    numeric_imputer = SimpleImputer(strategy='mean')
    {% elif preprocessing.missing_values == 'mode' %}
    numeric_imputer = SimpleImputer(strategy='most_frequent')
    {% else %}
    numeric_imputer = SimpleImputer(strategy='median')
    {% endif %}

    {% if preprocessing.scaling == 'standard' %}
    scaler = StandardScaler()
    {% elif preprocessing.scaling == 'minmax' %}
    scaler = MinMaxScaler()
    {% else %}
    scaler = 'passthrough'
    {% endif %}

    # Build numeric transformer
    if numeric_imputer == 'passthrough' and scaler == 'passthrough':
        numeric_transformer = 'passthrough'
    elif numeric_imputer == 'passthrough':
        numeric_transformer = scaler
    elif scaler == 'passthrough':
        numeric_transformer = numeric_imputer
    else:
        numeric_transformer = Pipeline([
            ('imputer', numeric_imputer),
            ('scaler', scaler)
        ])

    # Categorical preprocessing
    {% if preprocessing.encoding == 'ordinal' %}
    categorical_transformer = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))
    ])
    {% else %}
    categorical_transformer = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])
    {% endif %}

    # Combine transformers
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ],
        remainder='drop'
    )

    return preprocessor


def create_model():
    """Create the ML model."""
    {% block model %}
    raise NotImplementedError("Model creation not implemented")
    {% endblock %}


def calculate_metrics(y_true, y_pred, y_proba=None):
    """Calculate metrics based on task type."""
    metrics = {}

    {% if task_type == 'regression' %}
    metrics['rmse'] = float(np.sqrt(mean_squared_error(y_true, y_pred)))
    metrics['mae'] = float(mean_absolute_error(y_true, y_pred))
    metrics['r2'] = float(r2_score(y_true, y_pred))
    {% else %}
    metrics['accuracy'] = float(accuracy_score(y_true, y_pred))
    metrics['precision'] = float(precision_score(y_true, y_pred, average='weighted', zero_division=0))
    metrics['recall'] = float(recall_score(y_true, y_pred, average='weighted', zero_division=0))
    metrics['f1'] = float(f1_score(y_true, y_pred, average='weighted', zero_division=0))
    if y_proba is not None:
        try:
            metrics['log_loss'] = float(log_loss(y_true, y_proba))
        except Exception:
            pass
    {% endif %}

    return metrics


def main():
    """Run the experiment."""
    try:
        # Load data
        X, y = load_data()

        {% if preprocessing.missing_values == 'drop' %}
        # Drop rows with missing values
        mask = ~(X.isna().any(axis=1) | y.isna())
        X = X[mask]
        y = y[mask]
        {% endif %}

        {% if preprocessing.target_transform == 'log' and task_type == 'regression' %}
        # Log transform target
        y = np.log1p(y)
        {% endif %}

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
        )

        # Create preprocessor and model
        preprocessor = create_preprocessor(X_train)
        model = create_model()

        # Create full pipeline
        pipeline = Pipeline([
            ('preprocessor', preprocessor),
            ('model', model)
        ])

        # Train
        pipeline.fit(X_train, y_train)

        # Predict
        y_pred = pipeline.predict(X_test)

        {% if task_type == 'classification' %}
        # Get probabilities if available
        y_proba = None
        if hasattr(pipeline, 'predict_proba'):
            try:
                y_proba = pipeline.predict_proba(X_test)
            except Exception:
                pass
        metrics = calculate_metrics(y_test, y_pred, y_proba)
        {% else %}
        metrics = calculate_metrics(y_test, y_pred)

        {% if preprocessing.target_transform == 'log' %}
        # Calculate metrics on original scale
        y_test_orig = np.expm1(y_test)
        y_pred_orig = np.expm1(y_pred)
        metrics['rmse_original'] = float(np.sqrt(mean_squared_error(y_test_orig, y_pred_orig)))
        metrics['mae_original'] = float(mean_absolute_error(y_test_orig, y_pred_orig))
        {% endif %}
        {% endif %}

        # Cross-validation
        cv_scores = cross_val_score(
            pipeline, X, y,
            cv=5,
            scoring='neg_root_mean_squared_error' if TASK_TYPE == 'regression' else 'f1_weighted'
        )
        {% if task_type == 'regression' %}
        metrics['cv_rmse_mean'] = float(-cv_scores.mean())
        metrics['cv_rmse_std'] = float(cv_scores.std())
        {% else %}
        metrics['cv_f1_mean'] = float(cv_scores.mean())
        metrics['cv_f1_std'] = float(cv_scores.std())
        {% endif %}

        # Output results as JSON
        result = {
            "success": True,
            "metrics": metrics
        }
        print(json.dumps(result))

    except Exception as e:
        result = {
            "success": False,
            "error": str(e)
        }
        print(json.dumps(result))
        sys.exit(1)


if __name__ == "__main__":
    main()
